[{"authors":null,"categories":null,"content":"","date":1666569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696363102,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://ckids-datafest.github.io/2023-fall-nn-forgetting/people/","publishdate":"2022-10-24T00:00:00Z","relpermalink":"/2023-fall-nn-forgetting/people/","section":"","summary":"","tags":null,"title":"People","type":"landing"},{"authors":null,"categories":null,"content":"Data Preprocessing a) IID Data scenario : Each model was trained on a equal share of all the classes in the dataset.\nb) Non-IID Data scenario: We simulate the non-IID data distribution using the following logic:\nclient_a : (labels == \u0026#39;T-Shirt/top\u0026#39;) | (labels == \u0026#39;Trouser\u0026#39;) | (labels == \u0026#39;Pullover\u0026#39;) client_b : (labels == \u0026#39;Dress\u0026#39;) | (labels == \u0026#39;Coat\u0026#39;) client_c : (labels == \u0026#39;Sandal\u0026#39;) | (labels == \u0026#39;Shirt\u0026#39;) client_d : (labels == \u0026#39;Sneaker\u0026#39;) | (labels == \u0026#39;Bag\u0026#39;) | (labels == \u0026#39;Ankle Boot\u0026#39;) Model Development We used the following Convolutional Neural Network for all the clients and the community model : _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= fashion (InputLayer) [(None, 28, 28, 1)] 0 conv2d_240 (Conv2D) (None, 24, 24, 128) 3328 max_pooling2d_240 (MaxPool (None, 12, 12, 128) 0 ing2D) conv2d_241 (Conv2D) (None, 6, 6, 128) 802944 max_pooling2d_241 (MaxPool (None, 3, 3, 128) 0 ing2D) flatten_120 (Flatten) (None, 1152) 0 dense_120 (Dense) (None, 512) 590336 predictions (Dense) (None, 10) 5130 ================================================================= Total params: 1401738 (5.35 MB) Trainable params: 1401738 (5.35 MB) Non-trainable params: 0 (0.00 Byte) _________________________________________________________________ We used Keras wrapper on Tensorflow for implementing the models. For visualization we stuck to Matplotlib and Seaborn.\nFor the scaffold model, each model was also equipped with client control variate for each layer to adjust for the drift. Since we had the architecture of our model fixed before hand, we could keep the vector structures of the control variates also the same as the layer weights.\nclass ControlVariate(): def __init__(self): self.conv1_c = tensorflow.zeros((5,5,1,128)) self.conv2_c = tensorflow.zeros((7,7,128,128)) self.flatten1_c = tensorflow.zeros((1152,512)) self.flatten2_c = tensorflow.zeros((512,10)) self.pred_c = tensorflow.zeros((10,)) We implemented the suggestions laid down by the SCAFFOLD Paper with each mini-batch train step as mentioned in the algorithm outlined in the paper.\nModel Evaluation a) Evaluation of performance : The evaluation dataset has 10000 instances of all classes to test how the clients perform on the global dataset. This enables to check if the knowledge has been transferred merely through weight transmission.\nb) Evaluation of client drift: The objective of this metric was to measure the client drift. We wanted to answer the question : how to visualize the drift of a model from it’s optima? The model output is the function of the weights of the model. We take the softmax output of the last layer of the client and we extract two probability distributions: one before the local training (after the weights are distributed from the community model) and one after the local training. We then measure the KL Divergence of the two probability distributions to see how much the output distribution varies from the community model. This is essentially the catastrophic forgetting described in the earlier sections. Interesting, this is also problem in Transfer Learning: how much of the pre-trained knowledge is lost in the fine-tuning but in the case of transfer learning, it’s core purpose and evaluation metrics lies in domain of the finetuned data. We don’t worry that much about the pre-trained knowledge.\nc) Visualization of the class clustering: Within the dataset, certain instances pose significant challenges. To define this difficulty, consider the scenario where a shirt and a pullover exhibit remarkable similarity. The model must discern subtle distinctions between them. Our aim was to visually represent this clustering by examining the neural network’s weight. Following each round, we fed a segment of the validation dataset through the network, capturing the activations from the final Dense layer of the CNN. We then reduced the dimensionality from 512 to 2 using t-SNE and plotted the resulting points after each round. The animated visualizations can be found in the Results section.\n","date":1530144e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701922965,"objectID":"3e050718825977bdecb55c075afa314a","permalink":"https://ckids-datafest.github.io/2023-fall-nn-forgetting/approach/","publishdate":"2018-06-28T00:00:00Z","relpermalink":"/2023-fall-nn-forgetting/approach/","section":"","summary":"Data science methodology used to address the problem, including data preprocessing steps, exploratory data analysis, feature engineering techniques, machine learning models, and evaluation metrics.","tags":null,"title":"Approach","type":"page"},{"authors":null,"categories":null,"content":"Introduction Since we were dealing with a very fundamental issue, we picked a very simple dataset to first reproduce the issue and later research ways on how to fix it.\nData Overview and Examples We used the Fashion mnist dataset for this problem statement. We also tried to run the system with Cifar-10 dataset. Despite being fairly simplistic in nature, these datasets can be engineered to get a non-IID and IID fractions. Very simply put, IID stands for data that comes from independent, identically distributed dataset. Non-IID is the exact opposite of that. This dataset doesn’t share a common distribution. In real life Machine Learning problem statement, the dataset is predominantly Non-IID and also we generally don’t know the true underlying distribution of the data. So it is very important that we truly understand this problem and make attempts to solve it. IID Non-IID This is simply how we simulate IID and Non-IID dataset from Fashion Mnist data.\n","date":1530144e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1702717406,"objectID":"be566fdb6f0fa08cfea50d77a89a6b5a","permalink":"https://ckids-datafest.github.io/2023-fall-nn-forgetting/data/","publishdate":"2018-06-28T00:00:00Z","relpermalink":"/2023-fall-nn-forgetting/data/","section":"","summary":"Data Assessment Document that gives an overview of the data used for the project.","tags":null,"title":"Data Assessment","type":"page"},{"authors":null,"categories":null,"content":"Introduction Federated Learning originates from the rising concerns about data privacy and the constraints of conventional centralized machine learning approaches. It emerged as a solution to safeguard user privacy while leveraging the wealth of knowledge found in dispersed data sources.\nIndustries increasingly embraced this concept upon realizing its potential for cooperative learning within the bounds of data privacy regulations. Its rapid evolution owes much to progress in encryption methods, communication protocols, and decentralized optimization algorithms. Presently, Federated Learning represents an encouraging framework that facilitates collaboration among diverse entities, prioritizing the protection of confidential user information and driving advancements in privacy-focused machine learning.\nMotivation Prof. Abram wanted us to address a very specific scenario. In the original Fed avg. paper, there is a recommendation put forward that the weights from the community be overwritten at the local client site. Now consider the example of a hospital with multiple sites and there is a model that predicts a condition based on certain characteristics. This case can be Non-IID in the sense that a form of condition might have never been encountered at a site. If we can use federated learning to solve this problem, then the sensitivity of the patient’s privacy at a differnt client is maintained and the characteristics are learnt without any network overhead. But in Non-IID case, there is catastrophic forgetting at the client level. The task completely pulls the community weights in the direction of the local minima!\nUnder IID conditions, Under Non-IID conditions, Our motivation for the semester was two folds : First, to understand the drift of the community model and second to visualize and understand the catastrophic forgetting exhibited by the client models.\nProblem for the Semester For the semester, we had two simple goals : To establish a working federated learning network - a problem statement rooted in Engineering and the second one was to visualized how the clients drifted - to answer the question : how does one quantify this drift?\nState of the Art We initially sought to reproduce and test the network proposed in FedSiM. We shall report the findings in the subsequent sections. We were also fascinated by the amazing math based proof for fixing client drifts in SCAFFOLD: Stochastic Controlled Averaging for Federated Learning and wanted to implement the paper’s proposed system.\nDesign and Approach Though initially framed as an engineering problem, at its essence, this constituted a research project aimed at exploring and executing various academic papers to evaluate their effectiveness. Consequently, we implemented two noteworthy papers over the semester to analyze and observe their outcomes. (1) For FedSiM paper : We built a network of Convolution Neural Networks(clients) and a Community model. Like it is proposed in the paper, we manipulated each neuron in every layer of the network with a small value to observe its overall impact on the output of the network. (2) For the SCAFFOLD paper : We built a Federated network of convolutional neural networks and a community model. Each model was equipped with a client control variate as proposed in the paper to account and adjust for the client drift.\n","date":1530144e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1702694173,"objectID":"b7c3446bb0d5d7e8a477294017361379","permalink":"https://ckids-datafest.github.io/2023-fall-nn-forgetting/problem-statement/","publishdate":"2018-06-28T00:00:00Z","relpermalink":"/2023-fall-nn-forgetting/problem-statement/","section":"","summary":"Problem and Requirements document that will drive the work to be done in the project","tags":null,"title":"Problem and Requirements","type":"page"},{"authors":null,"categories":null,"content":"Visualization of the last layer of the model So how does the softmax layer give out the classes to be predicted. It takes the activations from the last layer of the model. We took a specific class-balanced subset of the test dataset and pulled out the activations from the last layer of the model. After reducing the dimension of the activation for an instance to 2 using t-SNE, we plotted the each instance colored by its target label. If the classes are placed well apart, it visually shows that even in two dimensions, these class clusters can be separated. Not a very accurate way but makes up for a visual. After all, we are very visual beings. :)\nIID Non-IID As the rounds proceed, there is clear separation in the IID-cases but in the non-IID case, there is still a lot of overlap.\nKL Divergence KL Divergence can show the disimilarity between two probability distributions. The final output is the function of the weights of the models. So we propose using KL Divergence as a measure of the catastrophic forgetting happening at the client sites. Specifically, KL Divergence between the probability distributions obtained from the community model and the local model after a round of finetuning is taken. A low divergence score is a good thing!\nIID Non-IID Validation accuracy plots We plotted the validation set accuracy for the community model as well as the clients. For the clients, the validation set contains all the classes and not just the classes it has in its local site.\nIID Non-IID This plot essentially brings out the two issues we are trying to address. First, the validation accuracy of the community model is off by 10% in case of Non-IID data, which shows a drift in the community model. Second, the local clients never do that well on the validation set! It has poor accuracy. These are the two issues we sought to visualize and fix, if possible!\nScaffold Scaffold addresses the issue by adjusting for the client drift.\nValidation plot KL Divergence Discussion of Findings While there may not be significant fixes at this stage, the semester long research into understanding the problem statement gave us two prominent papers and how they fail to work with our problem statement.\n(i) At this point, we believe with scaffold, the pulling back of the client models at each local step prevents it from capturing the intricacies of the dataset locally thus affecting the performance of the global community model. (ii) We investigated how this drift happens at each layer and found the first layer to be the most divergent. While this could be attributed to the fact that the input layer changes most with introduction of new data (global validation to local clients), a counter argument can also be made that the first layer picks up basic features and as the layers progresses, the features get accumulated to get a bigger picture. ","date":1530144e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1702717847,"objectID":"c29e41198fe1dc5c85e66dbe4f2d7737","permalink":"https://ckids-datafest.github.io/2023-fall-nn-forgetting/results/","publishdate":"2018-06-28T00:00:00Z","relpermalink":"/2023-fall-nn-forgetting/results/","section":"","summary":"The main results of the work done to date","tags":null,"title":"Results","type":"page"}]